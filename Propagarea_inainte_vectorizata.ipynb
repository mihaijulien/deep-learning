{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Propagarea inainte vectorizata.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeapclORPlpbV4bqx6DDS7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ISzbp_Z_Xam6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "x9bcE2A0XtPH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN():\n",
        "    \"\"\"\n",
        "    Varianta definire MLP cu specificare input si output pentru fiecare unitate.\n",
        "    \"\"\"\n",
        "    def __init__(self, architecture, random_seed=42):\n",
        "        np.random.seed(random_seed)\n",
        "        \n",
        "        self.activation_functions = {\n",
        "            'sigmoid': sigmoid,\n",
        "            'relu': relu\n",
        "        }\n",
        "        \n",
        "        number_of_layers = len(architecture)\n",
        "        params_values = {}\n",
        "\n",
        "        for idx, layer in enumerate(architecture):\n",
        "            layer_idx = idx + 1\n",
        "            \n",
        "            layer_input_size = layer[\"input_dim\"]\n",
        "            \n",
        "            layer_output_size = layer[\"units\"]\n",
        "\n",
        "            params_values['W' + str(layer_idx)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
        "            \n",
        "            params_values['b' + str(layer_idx)] = np.random.randn(layer_output_size, 1) * 0.1\n",
        "            \n",
        "            \n",
        "        self.params = params_values\n",
        "        self.architecture = architecture\n",
        "        \n",
        "    def summary(self):\n",
        "        print(\"{:^15s} {:^15s} {:^15s} {:^15s} {:^15s} {:^15s} \\n\".format(\n",
        "            \"Input shape\",\n",
        "            \"Output shape\",\n",
        "            \"Weights shape\",\n",
        "            \"Bias shape\",\n",
        "            \"Activation\",\n",
        "            \"Params\"\n",
        "        ))\n",
        "        print(\"{:_<100s}\".format(''))\n",
        "        total_params = 0\n",
        "        for idx, layer in enumerate(self.architecture):\n",
        "            layer_idx = idx + 1\n",
        "            in_shape = layer[\"input_dim\"]\n",
        "            out_shape = layer[\"units\"]\n",
        "            \n",
        "            weights_shape = self.params['W' + str(layer_idx)].shape\n",
        "            bias_shape = self.params['b' + str(layer_idx)].shape\n",
        "            \n",
        "            activation = layer[\"activation\"]\n",
        "            \n",
        "            weights_params = 1\n",
        "            for dim in weights_shape:\n",
        "                weights_params *= dim\n",
        "            \n",
        "            bias_params = 1\n",
        "            for dim in bias_shape:\n",
        "                bias_params *= dim\n",
        "            \n",
        "            num_params = bias_params + weights_params\n",
        "            total_params += num_params\n",
        "            \n",
        "            print(\"{:^15d} {:^15d} {:^15s} {:^15s} {:^15s} {:^15d} \\n\".format(\n",
        "                in_shape,\n",
        "                out_shape,\n",
        "                str(weights_shape),\n",
        "                str(bias_shape),\n",
        "                activation,\n",
        "                num_params\n",
        "            ))\n",
        "            print(\"-\"*100)\n",
        "            \n",
        "        print(\"Total number of parameters: {}\".format(total_params))\n",
        "\n",
        "    def forward(self, w, b, x, activation='relu'):\n",
        "        z = np.dot(w, x) + b\n",
        "    \n",
        "        return self.activation_functions[activation](z), z\n",
        "        \n",
        "    def predict(self, x):\n",
        "        \"\"\" Functie cu care realizam predictii prin metoda propagarii inainte. \"\"\"\n",
        "        memory = {}\n",
        "        current_activation = x\n",
        "\n",
        "        for idx, layer in enumerate(self.architecture):\n",
        "            layer_idx = idx + 1\n",
        "            previous_activation = current_activation\n",
        "            activation_function = layer[\"activation\"]\n",
        "            w = self.params[\"W\" + str(layer_idx)]\n",
        "            b = self.params[\"b\" + str(layer_idx)]\n",
        "\n",
        "            current_activation, z = self.forward(w, b, current_activation, activation_function)\n",
        "\n",
        "            memory[\"x\" + str(idx)] = previous_activation\n",
        "            memory[\"z\" + str(layer_idx)] = z\n",
        "\n",
        "        return current_activation, memory"
      ],
      "metadata": {
        "id": "uvT0nmnRX4st"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [\n",
        "    {\n",
        "        \"input_dim\": 2,\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\"\n",
        "    },\n",
        "    {\n",
        "        \"input_dim\": 4,\n",
        "        \"units\": 6,\n",
        "        \"activation\": \"relu\"\n",
        "    },\n",
        "    {\n",
        "        \"input_dim\": 6,\n",
        "        \"units\": 6,\n",
        "        \"activation\": \"relu\"\n",
        "    },\n",
        "    {\n",
        "        \"input_dim\": 6,\n",
        "        \"units\": 4,\n",
        "        \"activation\": \"relu\"\n",
        "    },\n",
        "    {\n",
        "        \"input_dim\": 4,\n",
        "        \"units\": 1,\n",
        "        \"activation\": \"sigmoid\"\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "IJyiWj8dYzm9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NN(architecture)"
      ],
      "metadata": {
        "id": "emU0_ugTY2hd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBZaGi7BdvJh",
        "outputId": "77cb3e1b-7006-4ea8-8a5e-750e94e6650d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Input shape    Output shape    Weights shape    Bias shape      Activation        Params      \n",
            "\n",
            "____________________________________________________________________________________________________\n",
            "       2               4            (4, 2)          (4, 1)           relu             12        \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "       4               6            (6, 4)          (6, 1)           relu             30        \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "       6               6            (6, 6)          (6, 1)           relu             42        \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "       6               4            (4, 6)          (4, 1)           relu             28        \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "       4               1            (1, 4)          (1, 1)          sigmoid            5        \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Total number of parameters: 117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(2, 10)"
      ],
      "metadata": {
        "id": "9InA2Y2cd4sE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, mem = nn.predict(x)"
      ],
      "metadata": {
        "id": "BBZgj_hedy97"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stvePgx5d7DT",
        "outputId": "925bbd53-1f21-44df-faea-00504760800c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0p6BDiId8AH",
        "outputId": "3a5ff298-a3f3-49b1-9b08-f13b8ae04019"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49936672, 0.4993649 , 0.49936576, 0.49936671, 0.49936659,\n",
              "        0.49936564, 0.49936593, 0.49936672, 0.49936757, 0.49936672]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}